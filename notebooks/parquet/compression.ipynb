{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instalar Parquet, Pandas e Pyarrow\n",
    "%!pip install parquet\n",
    "%!pip install pandas\n",
    "%!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importar os pacotes necessários\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do arquivo CSV original: 14704853 bytes\n"
     ]
    }
   ],
   "source": [
    "csv_file_path = '../../databases/csv/PFW_2021_public.csv'\n",
    "output_dir = 'dataset/compression'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Obtém o tamanho do arquivo CSV original para usar como base\n",
    "try:\n",
    "    tamanho_csv = os.path.getsize(csv_file_path)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Erro: Arquivo CSV original '{csv_file_path}' não encontrado. A taxa de compressão não será calculada.\")\n",
    "    tamanho_csv = None # Define como None para evitar erros depois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do arquivo CSV original: 14704853 bytes\n",
      "Arquivo Parquet com compressão sem compressão criado em 0.0601 segundos.\n",
      "Tamanho do arquivo: 4725428 bytes\n",
      "Taxa de redução em relação ao CSV: 67.86%\n",
      "Arquivo Parquet com compressão snappy criado em 0.0687 segundos.\n",
      "Tamanho do arquivo: 3307142 bytes\n",
      "Taxa de redução em relação ao CSV: 77.51%\n"
     ]
    }
   ],
   "source": [
    "def criar_parquet_com_compressao(csv_path, output_path, compression='snappy', tamanho_csv=None):\n",
    "    \"\"\"\n",
    "    Lê um CSV, converte para Parquet com compressão, mede tempo, tamanho e calcula taxa de redução em relação ao CSV.\n",
    "\n",
    "    Args:\n",
    "        csv_path (str): Caminho para o CSV.\n",
    "        output_path (str): Caminho para o Parquet de saída.\n",
    "        compression (str, optional): Tipo de compressão. Padrão: 'snappy'.\n",
    "        tamanho_csv (int, optional): Tamanho do arquivo CSV para cálculo da taxa de redução.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        table = pa.Table.from_pandas(df)\n",
    "\n",
    "        start_time = time.time()\n",
    "        pq.write_table(table, output_path, compression=compression)\n",
    "        end_time = time.time()\n",
    "\n",
    "        tamanho_arquivo = os.path.getsize(output_path)\n",
    "        print(f\"Arquivo Parquet com compressão {compression if compression else 'sem compressão'} criado em {end_time - start_time:.4f} segundos.\")\n",
    "        print(f\"Tamanho do arquivo: {tamanho_arquivo} bytes\")\n",
    "\n",
    "        if tamanho_csv:\n",
    "            taxa_compressao = (tamanho_csv - tamanho_arquivo) / tamanho_csv * 100\n",
    "            print(f\"Taxa de redução em relação ao CSV: {taxa_compressao:.2f}%\") # Mensagem alterada\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Erro: Arquivo '{csv_path}' não encontrado.\")\n",
    "    except pa.ArrowInvalid as e:\n",
    "        print(f\"Erro na conversão para PyArrow: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ocorreu um erro ao escrever o Parquet: {e}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    print(f\"Tamanho do arquivo CSV original: {tamanho_csv} bytes\")\n",
    "\n",
    "\n",
    "    output_parquet_path_sem_compressao = os.path.join(output_dir, 'PFW_2021_public_sem_compressao.parquet')\n",
    "    criar_parquet_com_compressao(csv_file_path, output_parquet_path_sem_compressao, compression=None, tamanho_csv=tamanho_csv)\n",
    "\n",
    "    output_parquet_path_snappy = os.path.join(output_dir, 'PFW_2021_public_snappy.parquet')\n",
    "    criar_parquet_com_compressao(csv_file_path, output_parquet_path_snappy, compression='snappy', tamanho_csv=tamanho_csv)\n",
    "\n",
    "    # output_parquet_path_gzip = os.path.join(output_dir, 'PFW_2021_public_gzip.parquet')\n",
    "    # criar_parquet_com_compressao(csv_file_path, output_parquet_path_gzip, compression='gzip', tamanho_csv=tamanho_csv)\n",
    "\n",
    "    # output_parquet_path_brotli = os.path.join(output_dir, 'PFW_2021_public_brotli.parquet')\n",
    "    # criar_parquet_com_compressao(csv_file_path, output_parquet_path_brotli, compression='brotli', tamanho_csv=tamanho_csv)\n",
    "\n",
    "    # output_parquet_path_zstd = os.path.join(output_dir, 'PFW_2021_public_zstd.parquet')\n",
    "    # criar_parquet_com_compressao(csv_file_path, output_parquet_path_zstd, compression='zstd', tamanho_csv=tamanho_csv)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # criando o diretório do arquivo titanic.parquet\n",
    "    df_parquet_path = '../../databases/parquet/PFW_2021_public.parquet'\n",
    "\n",
    "    # criando o diretório de destino dos arquivos de compressão, caso não exista\n",
    "    # output_dir = 'dataset/compression'\n",
    "    # os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    output_path = 'dataset/compression'\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    # ler os dados do arquivo csv para um DataFrame no Pandas\n",
    "    df_csv = pd.read_csv('../../databases/csv/titanic.csv')\n",
    "\n",
    "    # Converter para tabela PyArrow\n",
    "    table = pa.Table.from_pandas(df_csv)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Erro: Arquivo '{df_parquet_path}' não encontrado.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ocorreu um erro: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compressão de arquivo Parquet com Snappy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprimir_parquet_snappy(input_path, output_path):\n",
    "    \"\"\"\n",
    "    Comprime um arquivo Parquet usando Snappy.\n",
    "\n",
    "    Args:\n",
    "        input_path (str): Caminho para o arquivo Parquet de entrada.\n",
    "        output_path (str): Caminho para o arquivo Parquet de saída (com compressão Snappy).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Ler o arquivo Parquet usando PyArrow\n",
    "        table = pq.read_table(input_path)\n",
    "\n",
    "        # Escrever o arquivo Parquet com compressão Snappy\n",
    "        pq.write_table(table, output_path, compression='snappy')\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        tamanho_original = os.path.getsize(input_path)\n",
    "        tamanho_comprimido = os.path.getsize(output_path)\n",
    "        taxa_compressao = (tamanho_original - tamanho_comprimido) / tamanho_original * 100\n",
    "\n",
    "        print(f\"Arquivo comprimido com Snappy em {end_time - start_time:.4f} segundos.\")\n",
    "        print(f\"Tamanho original: {tamanho_original} bytes\")\n",
    "        print(f\"Tamanho comprimido: {tamanho_comprimido} bytes\")\n",
    "        print(f\"Taxa de compressão: {taxa_compressao:.2f}%\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Erro: Arquivo '{input_path}' não encontrado.\")\n",
    "    except pa.ArrowInvalid as e:\n",
    "        print(f\"Erro ao ler o arquivo Parquet: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ocorreu um erro durante a compressão: {e}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Função principal.\"\"\"\n",
    "    input_parquet = '../../databases/parquet/PFW_2021_public.parquet'  # Substitua pelo caminho do seu arquivo Parquet\n",
    "    output_parquet_snappy = 'dataset/PFW_2021_public_snappy.parquet' # Substitua pelo caminho de saida desejado\n",
    "    output_dir = 'dataset'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    if not os.path.exists(input_parquet):\n",
    "        print(f\"Erro: Arquivo Parquet original '{input_parquet}' não encontrado.\")\n",
    "        return\n",
    "\n",
    "    comprimir_parquet_snappy(input_parquet, output_parquet_snappy)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparação de compressão em arquivos Parquet - snappy | gzip | brotli | zstd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Definindo as opções de compressão\n",
    "    compressions = {\n",
    "        'sem_compressao': None,\n",
    "        'snappy': 'snappy',\n",
    "        'gzip': 'gzip',\n",
    "        'brotli': 'brotli',\n",
    "        'zstd': 'zstd',\n",
    "        # 'lzo': 'lzo'\n",
    "    }\n",
    "\n",
    "    #\n",
    "    resultados = {}\n",
    "    for nome, compressao in compressions.items():\n",
    "        nome_arquivo = os.path.join(output_dir, f'titanic_{nome}.parquet')\n",
    "        start_time_escrita = time.time()\n",
    "        pq.write_table(table, nome_arquivo, compression=compressao)\n",
    "        end_time_escrita = time.time()\n",
    "        tamanho_arquivo = os.path.getsize(nome_arquivo)\n",
    "\n",
    "        start_time_leitura = time.time()\n",
    "        pd.read_parquet(nome_arquivo)  # Lê o arquivo para medir o tempo de leitura.\n",
    "        end_time_leitura = time.time()\n",
    "\n",
    "        resultados[nome] = {\n",
    "            'tempo_escrita': end_time_escrita - start_time_escrita,\n",
    "            'tempo_leitura': end_time_leitura - start_time_leitura,\n",
    "            'tamanho': tamanho_arquivo  # Mantém o tamanho aqui\n",
    "        }\n",
    "\n",
    "    print(\"Resultados da comparação de compressão:\\n\")\n",
    "    print(\"{:<15} {:<15} {:<15} {:<15}\".format(\"Compressão\", \"Tempo Escrita\", \"Tempo Leitura\", \"Tamanho (bytes)\")) # Ajuste na ordem dos cabeçalhos\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for nome, dados in resultados.items():\n",
    "        print(\"{:<15} {:<15.4f} {:<15.4f} {:<15}\".format( # Ajuste na ordem dos dados\n",
    "            nome, dados['tempo_escrita'], dados['tempo_leitura'], dados['tamanho']\n",
    "        ))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Ocorreu um erro: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Ler os dados do arquivo CSV para um DataFrame do Pandas\n",
    "    df = pd.read_csv('../../databases/csv/titanic.csv')\n",
    "\n",
    "    # Converter o DataFrame do Pandas para uma tabela PyArrow\n",
    "    table = pa.Table.from_pandas(df)\n",
    "\n",
    "    # Definir as opções de gravação com compressão Snappy\n",
    "    parquet_file = os.path.join(output_dir, 'titanic_snappy.parquet')\n",
    "    start_time = time.time()\n",
    "    pq.write_table(table, parquet_file, compression='snappy')\n",
    "    end_time = time.time()\n",
    "    print(f\"Arquivo Parquet com compressão Snappy criado em {end_time - start_time:.4f} segundos.\")\n",
    "    print(f\"Tamanho do arquivo: {os.path.getsize(parquet_file)} bytes\")\n",
    "\n",
    "    # Criando o arquivo Parquet sem compressão\n",
    "    parquet_file_sem_compressao = os.path.join(output_dir, 'titanic_sem_compressao.parquet')\n",
    "    start_time = time.time()\n",
    "    pq.write_table(table, parquet_file_sem_compressao)\n",
    "    end_time = time.time()\n",
    "    print(f\"Arquivo Parquet SEM compressão criado em {end_time - start_time:.4f} segundos.\")\n",
    "    print(f\"Tamanho do arquivo: {os.path.getsize(parquet_file_sem_compressao)} bytes\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Erro: Arquivo '../../databases/csv/titanic.csv' não encontrado. Certifique-se de que o arquivo existe no diretório correto.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ocorreu um erro: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Criando o arquivo Parquet com compressão Gzip\n",
    "    parquet_file_gzip = os.path.join(output_dir, 'titanic_gzip.parquet')\n",
    "    start_time = time.time()\n",
    "    pq.write_table(table, parquet_file_gzip, compression='gzip')\n",
    "    end_time = time.time()\n",
    "    print(f\"Arquivo Parquet com compressão Gzip criado em {end_time - start_time:.4f} segundos.\")\n",
    "    print(f\"Tamanho do arquivo: {os.path.getsize(parquet_file_gzip)} bytes\")\n",
    "\n",
    "    # Criando o arquivo Parquet com compressão Brotli\n",
    "    parquet_file_brotli = os.path.join(output_dir, 'titanic_brotli.parquet')\n",
    "    start_time = time.time()\n",
    "    pq.write_table(table, parquet_file_brotli, compression='brotli')\n",
    "    end_time = time.time()\n",
    "    print(f\"Arquivo Parquet com compressão Brotli criado em {end_time - start_time:.4f} segundos.\")\n",
    "    print(f\"Tamanho do arquivo: {os.path.getsize(parquet_file_brotli)} bytes\")\n",
    "except Exception as e:\n",
    "    print(f\"Ocorreu um erro: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criar_parquet_com_compressao(input_path, output_path, compression='snappy'):\n",
    "    \"\"\"\n",
    "    Lê um arquivo Parquet, salva uma cópia com compressão especificada e mede o tempo e tamanho.\n",
    "\n",
    "    Args:\n",
    "        input_path (str): Caminho para o arquivo Parquet de entrada.\n",
    "        output_path (str): Caminho para o arquivo Parquet de saída.\n",
    "        compression (str, optional): Tipo de compressão ('snappy', 'gzip', 'brotli', 'zstd', None). Padrão é 'snappy'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Ler o Parquet original usando PyArrow diretamente para evitar conversão desnecessária\n",
    "        table = pq.read_table(input_path)\n",
    "\n",
    "        pq.write_table(table, output_path, compression=compression)\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        tamanho_arquivo = os.path.getsize(output_path)\n",
    "        print(f\"Arquivo Parquet com compressão {compression if compression else 'sem compressão'} criado em {end_time - start_time:.4f} segundos.\")\n",
    "        print(f\"Tamanho do arquivo: {tamanho_arquivo} bytes\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Erro: Arquivo '{input_path}' não encontrado.\")\n",
    "    except pa.ArrowInvalid as e:\n",
    "        print(f\"Erro ao ler o arquivo Parquet: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ocorreu um erro ao escrever o Parquet: {e}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Função principal para executar a aplicação de compressão.\"\"\"\n",
    "    input_parquet_path = '../../databases/parquet/titanic.parquet' # Caminho para o Parquet ORIGINAL\n",
    "    output_dir = 'dataset/compression'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Verifica se o arquivo parquet original existe\n",
    "    if not os.path.exists(input_parquet_path):\n",
    "        print(f\"Erro: Arquivo Parquet original '{input_parquet_path}' não encontrado. Certifique-se de que ele exista.\")\n",
    "        return # Encerra a execução se o arquivo não existe\n",
    "\n",
    "    compressions = {\n",
    "        'sem_compressao': None,\n",
    "        'snappy': 'snappy',\n",
    "        'gzip': 'gzip',\n",
    "        'brotli': 'brotli',\n",
    "        'zstd': 'zstd'\n",
    "    }\n",
    "\n",
    "    for nome, compressao in compressions.items():\n",
    "        output_parquet_path = os.path.join(output_dir, f'titanic_{nome}.parquet')\n",
    "        criar_parquet_com_compressao(input_parquet_path, output_parquet_path, compression=compressao)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprimir_parquet_snappy(input_path, output_path):\n",
    "    \"\"\"\n",
    "    Comprime um arquivo Parquet usando Snappy.\n",
    "\n",
    "    Args:\n",
    "        input_path (str): Caminho para o arquivo Parquet de entrada.\n",
    "        output_path (str): Caminho para o arquivo Parquet de saída (com compressão Snappy).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Ler o arquivo Parquet usando PyArrow\n",
    "        table = pq.read_table(input_path)\n",
    "\n",
    "        # Escrever o arquivo Parquet com compressão Snappy\n",
    "        pq.write_table(table, output_path, compression='snappy')\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        tamanho_original = os.path.getsize(input_path)\n",
    "        tamanho_comprimido = os.path.getsize(output_path)\n",
    "        taxa_compressao = (tamanho_original - tamanho_comprimido) / tamanho_original * 100\n",
    "\n",
    "        print(f\"Arquivo comprimido com Snappy em {end_time - start_time:.4f} segundos.\")\n",
    "        print(f\"Tamanho original: {tamanho_original} bytes\")\n",
    "        print(f\"Tamanho comprimido: {tamanho_comprimido} bytes\")\n",
    "        print(f\"Taxa de compressão: {taxa_compressao:.2f}%\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Erro: Arquivo '{input_path}' não encontrado.\")\n",
    "    except pa.ArrowInvalid as e:\n",
    "        print(f\"Erro ao ler o arquivo Parquet: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ocorreu um erro durante a compressão: {e}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Função principal.\"\"\"\n",
    "    input_parquet = '../../databases/parquet/titanic.parquet'  # Substitua pelo caminho do seu arquivo Parquet\n",
    "    output_parquet_snappy = 'dataset/titanic_snappy.parquet' # Substitua pelo caminho de saida desejado\n",
    "    output_dir = 'dataset'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    if not os.path.exists(input_parquet):\n",
    "        print(f\"Erro: Arquivo Parquet original '{input_parquet}' não encontrado.\")\n",
    "        return\n",
    "\n",
    "    comprimir_parquet_snappy(input_parquet, output_parquet_snappy)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

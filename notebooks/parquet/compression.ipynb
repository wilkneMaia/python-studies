{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requisitos\n",
    "Certifique-se de que os pacotes necessários estão instalados. Execute o seguinte comando:\n",
    "```bash\n",
    "pip install pandas pyarrow parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importar os pacotes necessários\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCaminho para o arquivo Parquet com compressão Brotli.\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Caminho do arquivo CSV original\n",
    "csv_file_path = '../../databases/csv/PFW_2021_public.csv'\n",
    "\"\"\"\n",
    "Caminho para o arquivo CSV de entrada. Contém os dados a serem convertidos para Parquet.\n",
    "\"\"\"\n",
    "\n",
    "# Caminho de destino onde os arquivos Parquet serão armazenados\n",
    "output_dir = 'dataset/compression'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\"\"\"\n",
    "Diretório onde os arquivos Parquet serão salvos. O diretório será criado se não existir.\n",
    "\"\"\"\n",
    "\n",
    "# Caminhos para os arquivos Parquet com diferentes compressões\n",
    "input_parquet = os.path.join(output_dir, 'sem_compressao.parquet')\n",
    "\"\"\"\n",
    "Caminho para o arquivo Parquet sem compressão.\n",
    "\"\"\"\n",
    "\n",
    "output_parquet_snappy = os.path.join(output_dir, 'snappy.parquet')\n",
    "\"\"\"\n",
    "Caminho para o arquivo Parquet com compressão Snappy.\n",
    "\"\"\"\n",
    "\n",
    "# Caminhos comentados para outros tipos de compressão\n",
    "# output_parquet_gzip = os.path.join(output_dir, 'gzip.parquet')\n",
    "\"\"\"\n",
    "Caminho para o arquivo Parquet com compressão Gzip.\n",
    "\"\"\"\n",
    "\n",
    "# output_parquet_brotli = os.path.join(output_dir, 'brotli.parquet')\n",
    "\"\"\"\n",
    "Caminho para o arquivo Parquet com compressão Brotli.\n",
    "\"\"\"\n",
    "\n",
    "# output_parquet_zstd = os.path.join(output_dir, 'zstd.parquet')\n",
    "\"\"\"\n",
    "Caminho para o arquivo Parquet com compressão Zstd.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compressão de arquivo CSV em Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do arquivo CSV original: 14704853 bytes\n",
      "Arquivo Parquet SEM compressão criado em 0.0567 segundos.\n",
      "Tamanho do arquivo: 4725428 bytes\n",
      "Taxa de redução em relação ao CSV: 67.86%\n"
     ]
    }
   ],
   "source": [
    "def criar_parquet_sem_compressao(csv_path, output_path, tamanho_csv=None):\n",
    "    \"\"\"\n",
    "    Lê um CSV, converte para Parquet sem compressão, mede o tempo de execução,\n",
    "    o tamanho do arquivo Parquet gerado e calcula a taxa de compressão em relação ao CSV.\n",
    "\n",
    "    Args:\n",
    "        csv_path (str): Caminho para o arquivo CSV.\n",
    "        output_path (str): Caminho de saída para o arquivo Parquet.\n",
    "        tamanho_csv (int, optional): Tamanho do arquivo CSV original para calcular a taxa de compressão.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Lê o arquivo CSV para um DataFrame\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "        # Converte o DataFrame para uma Tabela PyArrow\n",
    "        table = pa.Table.from_pandas(df)\n",
    "\n",
    "        # Medir o tempo de execução para escrever o arquivo Parquet\n",
    "        start_time = time.time()\n",
    "        pq.write_table(table, output_path, compression=None)  # Sem compressão\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Obtém o tamanho do arquivo Parquet gerado\n",
    "        tamanho_arquivo = os.path.getsize(output_path)\n",
    "\n",
    "        # Exibe o tempo e o tamanho do arquivo\n",
    "        print(f\"Arquivo Parquet SEM compressão criado em {end_time - start_time:.4f} segundos.\")\n",
    "        print(f\"Tamanho do arquivo: {tamanho_arquivo} bytes\")\n",
    "\n",
    "        # Calcula a taxa de compressão se o tamanho do arquivo CSV for fornecido\n",
    "        if tamanho_csv:\n",
    "            taxa_compressao = (tamanho_csv - tamanho_arquivo) / tamanho_csv * 100\n",
    "            print(f\"Taxa de redução em relação ao CSV: {taxa_compressao:.2f}%\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Erro: Arquivo '{csv_path}' não encontrado.\")\n",
    "    except pa.ArrowInvalid as e:\n",
    "        print(f\"Erro na conversão para PyArrow: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ocorreu um erro ao escrever o Parquet: {e}\")\n",
    "\n",
    "def main():\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Obtém o tamanho do arquivo CSV original para usar como base para a taxa de compressão\n",
    "    try:\n",
    "        tamanho_csv = os.path.getsize(csv_file_path)\n",
    "        print(f\"Tamanho do arquivo CSV original: {tamanho_csv} bytes\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Erro: Arquivo CSV original '{csv_file_path}' não encontrado. A taxa de compressão não será calculada.\")\n",
    "        tamanho_csv = None\n",
    "\n",
    "    # Gerando o arquivo Parquet SEM compressão\n",
    "    output_parquet_path_sem_compressao = os.path.join(output_dir, 'sem_compressao.parquet')\n",
    "    criar_parquet_sem_compressao(csv_file_path, output_parquet_path_sem_compressao, tamanho_csv=tamanho_csv)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compressão de arquivo Parquet com Snappy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo comprimido com Snappy em 0.0849 segundos.\n",
      "Tamanho original: 4725428 bytes\n",
      "Tamanho comprimido: 3307142 bytes\n",
      "Taxa de compressão: 30.01%\n"
     ]
    }
   ],
   "source": [
    "def comprimir_parquet_snappy(input_path, output_path):\n",
    "    \"\"\"\n",
    "    Comprime um arquivo Parquet usando Snappy.\n",
    "\n",
    "    Args:\n",
    "        input_path (str): Caminho para o arquivo Parquet de entrada.\n",
    "        output_path (str): Caminho para o arquivo Parquet de saída (com compressão Snappy).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Ler o arquivo Parquet usando PyArrow\n",
    "        table = pq.read_table(input_path)\n",
    "\n",
    "        # Escrever o arquivo Parquet com compressão Snappy\n",
    "        pq.write_table(table, output_path, compression='snappy')\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        tamanho_original = os.path.getsize(input_path)\n",
    "        tamanho_comprimido = os.path.getsize(output_path)\n",
    "        taxa_compressao = (tamanho_original - tamanho_comprimido) / tamanho_original * 100\n",
    "\n",
    "        print(f\"Arquivo comprimido com Snappy em {end_time - start_time:.4f} segundos.\")\n",
    "        print(f\"Tamanho original: {tamanho_original} bytes\")\n",
    "        print(f\"Tamanho comprimido: {tamanho_comprimido} bytes\")\n",
    "        print(f\"Taxa de compressão: {taxa_compressao:.2f}%\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Erro: Arquivo '{input_path}' não encontrado.\")\n",
    "    except pa.ArrowInvalid as e:\n",
    "        print(f\"Erro ao ler o arquivo Parquet: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ocorreu um erro durante a compressão: {e}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Função principal.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    if not os.path.exists(input_parquet):\n",
    "        print(f\"Erro: Arquivo Parquet original '{input_parquet}' não encontrado.\")\n",
    "        return\n",
    "\n",
    "    comprimir_parquet_snappy(input_parquet, output_parquet_snappy)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparação de compressão em arquivos Parquet - snappy | gzip | brotli | zstd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do arquivo CSV original: 14704853 bytes\n",
      "\n",
      "Criando Parquet com compressão snappy...\n",
      "Arquivo Parquet com compressão snappy criado em 0.0719 segundos.\n",
      "Tamanho do arquivo: 3307142 bytes\n",
      "Taxa de redução em relação ao CSV: 77.51%\n",
      "\n",
      "Criando Parquet com compressão gzip...\n",
      "Arquivo Parquet com compressão gzip criado em 0.3265 segundos.\n",
      "Tamanho do arquivo: 2519028 bytes\n",
      "Taxa de redução em relação ao CSV: 82.87%\n",
      "\n",
      "Criando Parquet com compressão brotli...\n",
      "Arquivo Parquet com compressão brotli criado em 0.3859 segundos.\n",
      "Tamanho do arquivo: 2506136 bytes\n",
      "Taxa de redução em relação ao CSV: 82.96%\n",
      "\n",
      "Criando Parquet com compressão zstd...\n",
      "Arquivo Parquet com compressão zstd criado em 0.0745 segundos.\n",
      "Tamanho do arquivo: 2501013 bytes\n",
      "Taxa de redução em relação ao CSV: 82.99%\n"
     ]
    }
   ],
   "source": [
    "def criar_parquet_com_compressao(csv_path, output_path, compression, tamanho_csv=None):\n",
    "    \"\"\"\n",
    "    Lê um CSV, converte para Parquet com compressão especificada, mede o tempo de execução,\n",
    "    o tamanho do arquivo Parquet gerado e calcula a taxa de compressão em relação ao CSV.\n",
    "\n",
    "    Args:\n",
    "        csv_path (str): Caminho para o arquivo CSV.\n",
    "        output_path (str): Caminho de saída para o arquivo Parquet.\n",
    "        compression (str): Tipo de compressão (snappy, gzip, brotli, zstd).\n",
    "        tamanho_csv (int, optional): Tamanho do arquivo CSV original para calcular a taxa de compressão.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Lê o arquivo CSV para um DataFrame\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "        # Converte o DataFrame para uma Tabela PyArrow\n",
    "        table = pa.Table.from_pandas(df)\n",
    "\n",
    "        # Medir o tempo de execução para escrever o arquivo Parquet com compressão\n",
    "        start_time = time.time()\n",
    "        pq.write_table(table, output_path, compression=compression)\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Obtém o tamanho do arquivo Parquet gerado\n",
    "        tamanho_arquivo = os.path.getsize(output_path)\n",
    "\n",
    "        # Exibe o tempo e o tamanho do arquivo\n",
    "        print(f\"Arquivo Parquet com compressão {compression} criado em {end_time - start_time:.4f} segundos.\")\n",
    "        print(f\"Tamanho do arquivo: {tamanho_arquivo} bytes\")\n",
    "\n",
    "        # Calcula a taxa de compressão se o tamanho do arquivo CSV for fornecido\n",
    "        if tamanho_csv:\n",
    "            taxa_compressao = (tamanho_csv - tamanho_arquivo) / tamanho_csv * 100\n",
    "            print(f\"Taxa de redução em relação ao CSV: {taxa_compressao:.2f}%\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Erro: Arquivo '{csv_path}' não encontrado.\")\n",
    "    except pa.ArrowInvalid as e:\n",
    "        print(f\"Erro na conversão para PyArrow: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ocorreu um erro ao escrever o Parquet: {e}\")\n",
    "\n",
    "\n",
    "def comparar_compression(csv_path, output_dir, tamanho_csv=None):\n",
    "    \"\"\"\n",
    "    Compara os tipos de compressão: snappy, gzip, brotli, zstd.\n",
    "    Para cada compressão, mede tempo, tamanho do arquivo e taxa de compressão.\n",
    "\n",
    "    Args:\n",
    "        csv_path (str): Caminho para o arquivo CSV.\n",
    "        output_dir (str): Diretório de saída para os arquivos Parquet.\n",
    "        tamanho_csv (int, optional): Tamanho do arquivo CSV original para cálculo da taxa de compressão.\n",
    "    \"\"\"\n",
    "    # Tipos de compressão para comparar\n",
    "    compressions = ['snappy', 'gzip', 'brotli', 'zstd']\n",
    "\n",
    "    # Itera sobre cada tipo de compressão\n",
    "    for compression in compressions:\n",
    "        output_path = os.path.join(output_dir, f'{compression}.parquet')\n",
    "        print(f\"\\nCriando Parquet com compressão {compression}...\")\n",
    "\n",
    "        # Chama a função de criação de Parquet para cada compressão\n",
    "        criar_parquet_com_compressao(csv_path, output_path, compression, tamanho_csv=tamanho_csv)\n",
    "\n",
    "\n",
    "def main():\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Obtém o tamanho do arquivo CSV original para usar como base para a taxa de compressão\n",
    "    try:\n",
    "        tamanho_csv = os.path.getsize(csv_file_path)\n",
    "        print(f\"Tamanho do arquivo CSV original: {tamanho_csv} bytes\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Erro: Arquivo CSV original '{csv_file_path}' não encontrado. A taxa de compressão não será calculada.\")\n",
    "        tamanho_csv = None\n",
    "\n",
    "    # Comparar os arquivos Parquet com diferentes compressões\n",
    "    comparar_compression(csv_file_path, output_dir, tamanho_csv=tamanho_csv)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usando Compressão Automática com o PyArrow (sem especificar o tipo de compressão)\n",
    "\n",
    "O PyArrow suporta compressão automática para Parquet se nenhum tipo de compressão for especificado. Por padrão, ele escolherá a melhor compressão disponível dependendo da instalação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criar_parquet_com_compressao_automatica(csv_path, output_path):\n",
    "    \"\"\"\n",
    "    Lê um CSV, converte para Parquet com compressão automática.\n",
    "\n",
    "    Args:\n",
    "        csv_path (str): Caminho para o arquivo CSV.\n",
    "        output_path (str): Caminho de saída para o arquivo Parquet.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Lê o arquivo CSV para um DataFrame\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "        # Converte o DataFrame para uma Tabela PyArrow\n",
    "        table = pa.Table.from_pandas(df)\n",
    "\n",
    "        # Escreve o Parquet com compressão automática\n",
    "        pq.write_table(table, output_path)\n",
    "\n",
    "        # Obtém o tamanho do arquivo Parquet gerado\n",
    "        tamanho_arquivo = os.path.getsize(output_path)\n",
    "\n",
    "        # Exibe o tamanho do arquivo\n",
    "        print(f\"Arquivo Parquet com compressão automática criado. Tamanho do arquivo: {tamanho_arquivo} bytes\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao criar o Parquet: {e}\")\n",
    "\n",
    "# Exemplo de uso\n",
    "criar_parquet_com_compressao_automatica(csv_file_path, f'{output_dir}/PFW_2021_public_compressao_automatica.parquet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicação: Quando você não especifica a compressão, o PyArrow tentará usar o melhor método disponível para compressão. Dependendo do ambiente e da instalação, isso pode ser Snappy ou outro tipo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usando Compressão snappy e Compactação de Colunas Individualmente\n",
    "\n",
    "Se você tem um grande número de colunas no seu DataFrame, pode usar diferentes algoritmos de compressão para cada coluna. Aqui está como isso pode ser feito:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criar_parquet_com_compressao_individual(csv_path, output_path):\n",
    "    \"\"\"\n",
    "    Lê um CSV, converte para Parquet, aplica compressão individualmente por coluna.\n",
    "\n",
    "    Args:\n",
    "        csv_path (str): Caminho para o arquivo CSV.\n",
    "        output_path (str): Caminho de saída para o arquivo Parquet.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Lê o arquivo CSV para um DataFrame\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "        # Converte o DataFrame para uma Tabela PyArrow\n",
    "        table = pa.Table.from_pandas(df)\n",
    "\n",
    "        # Especifica compressão diferente por coluna (aqui, apenas como exemplo)\n",
    "        compression_opts = {\n",
    "            'Nome': 'snappy',  # Compressão snappy para a coluna 'Nome'\n",
    "            'Idade': 'gzip',   # Compressão gzip para a coluna 'Idade'\n",
    "            'Salario': 'zstd', # Compressão zstd para a coluna 'Salario'\n",
    "            'Data_Admissao': 'brotli', # Compressão brotli para a coluna 'Data_Admissao'\n",
    "        }\n",
    "\n",
    "        # Escreve o Parquet com compressão por coluna\n",
    "        pq.write_table(table, output_path, compression=compression_opts)\n",
    "\n",
    "        # Obtém o tamanho do arquivo Parquet gerado\n",
    "        tamanho_arquivo = os.path.getsize(output_path)\n",
    "\n",
    "        # Exibe o tamanho do arquivo\n",
    "        print(f\"Arquivo Parquet com compressão individual criada. Tamanho do arquivo: {tamanho_arquivo} bytes\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao criar o Parquet: {e}\")\n",
    "\n",
    "# Exemplo de uso\n",
    "criar_parquet_com_compressao_individual(csv_file_path, f'{output_dir}/compressao_colunas.parquet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicação: Nesse exemplo, usamos diferentes tipos de compressão em cada coluna. A coluna Nome usará snappy, Idade usará gzip, e assim por diante. Essa técnica pode ser útil quando você sabe que alguns dados (como strings) podem ser compactados mais eficientemente com certos algoritmos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script de Comparação de Compressões Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tipos de Compressão e quando usar:\n",
      "\n",
      "SNAPPY: Snappy é rápido e proporciona boa performance de leitura e escrita. Ideal para grandes volumes de dados quando a velocidade de leitura/escrita é mais importante do que a taxa de compressão. Usado comumente no Hadoop e Spark.\n",
      "\n",
      "GZIP: Gzip oferece uma compressão mais forte que o Snappy, mas com um custo maior em termos de velocidade de leitura e escrita. Ideal quando é necessário reduzir o tamanho do arquivo e quando o tempo de leitura/gravação não é crítico.\n",
      "\n",
      "BROTLI: Brotli é uma compressão moderna que oferece taxas de compressão superiores ao Gzip, mas tende a ser mais lenta. É uma boa escolha quando se deseja um bom equilíbrio entre compressão eficiente e desempenho.\n",
      "\n",
      "ZSTD: Zstd (Zstandard) oferece alta compressão e performance comparável ao Snappy. Ideal para quando se deseja a melhor taxa de compressão com tempos razoáveis de leitura/escrita, muito usado em ambientes modernos.\n",
      "\n",
      "Compressão snappy:\n",
      "Tempo de execução: 0.0727 segundos\n",
      "Tamanho do arquivo: 3307142 bytes\n",
      "\n",
      "Compressão gzip:\n",
      "Tempo de execução: 0.3300 segundos\n",
      "Tamanho do arquivo: 2519028 bytes\n",
      "\n",
      "Compressão brotli:\n",
      "Tempo de execução: 0.4328 segundos\n",
      "Tamanho do arquivo: 2506136 bytes\n",
      "\n",
      "Compressão zstd:\n",
      "Tempo de execução: 0.0745 segundos\n",
      "Tamanho do arquivo: 2501013 bytes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Função para salvar arquivo Parquet com diferentes tipos de compressão\n",
    "def salvar_parquet_com_compressao(csv_path, output_path, compression):\n",
    "    \"\"\"\n",
    "    Função que converte um CSV para Parquet com a compressão especificada.\n",
    "\n",
    "    Args:\n",
    "        csv_path (str): Caminho para o arquivo CSV de entrada.\n",
    "        output_path (str): Caminho de saída para o arquivo Parquet.\n",
    "        compression (str): Tipo de compressão (snappy, gzip, brotli, zstd).\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    table = pa.Table.from_pandas(df)\n",
    "\n",
    "    # Medir tempo de execução para compressão\n",
    "    start_time = time.time()\n",
    "    pq.write_table(table, output_path, compression=compression)\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Exibir o tempo de execução\n",
    "    tempo_execucao = end_time - start_time\n",
    "    tamanho_arquivo = os.path.getsize(output_path)\n",
    "\n",
    "    print(f\"Compressão {compression}:\\n\"\n",
    "          f\"Tempo de execução: {tempo_execucao:.4f} segundos\\n\"\n",
    "          f\"Tamanho do arquivo: {tamanho_arquivo} bytes\\n\")\n",
    "\n",
    "# Tipos de compressão e suas características:\n",
    "tipos_compressao = {\n",
    "    \"snappy\": \"Snappy é rápido e proporciona boa performance de leitura e escrita. \"\n",
    "              \"Ideal para grandes volumes de dados quando a velocidade de leitura/escrita é mais importante \"\n",
    "              \"do que a taxa de compressão. Usado comumente no Hadoop e Spark.\",\n",
    "\n",
    "    \"gzip\": \"Gzip oferece uma compressão mais forte que o Snappy, mas com um custo maior em termos de velocidade \"\n",
    "            \"de leitura e escrita. Ideal quando é necessário reduzir o tamanho do arquivo e quando o tempo \"\n",
    "            \"de leitura/gravação não é crítico.\",\n",
    "\n",
    "    \"brotli\": \"Brotli é uma compressão moderna que oferece taxas de compressão superiores ao Gzip, \"\n",
    "              \"mas tende a ser mais lenta. É uma boa escolha quando se deseja um bom equilíbrio entre \"\n",
    "              \"compressão eficiente e desempenho.\",\n",
    "\n",
    "    \"zstd\": \"Zstd (Zstandard) oferece alta compressão e performance comparável ao Snappy. \"\n",
    "            \"Ideal para quando se deseja a melhor taxa de compressão com tempos razoáveis de leitura/escrita, \"\n",
    "            \"muito usado em ambientes modernos.\"\n",
    "}\n",
    "\n",
    "# Exibindo as informações sobre cada tipo de compressão\n",
    "print(\"Tipos de Compressão e quando usar:\\n\")\n",
    "for tipo, descricao in tipos_compressao.items():\n",
    "    print(f\"{tipo.upper()}: {descricao}\\n\")\n",
    "\n",
    "# Caminhos dos arquivos Parquet com diferentes compressões\n",
    "output_parquet_snappy = os.path.join(output_dir, 'snappy.parquet')\n",
    "output_parquet_gzip = os.path.join(output_dir, 'gzip.parquet')\n",
    "output_parquet_brotli = os.path.join(output_dir, 'brotli.parquet')\n",
    "output_parquet_zstd = os.path.join(output_dir, 'zstd.parquet')\n",
    "\n",
    "# Salvar arquivos Parquet com compressão de cada tipo\n",
    "salvar_parquet_com_compressao(csv_file_path, output_parquet_snappy, 'snappy')\n",
    "salvar_parquet_com_compressao(csv_file_path, output_parquet_gzip, 'gzip')\n",
    "salvar_parquet_com_compressao(csv_file_path, output_parquet_brotli, 'brotli')\n",
    "salvar_parquet_com_compressao(csv_file_path, output_parquet_zstd, 'zstd')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

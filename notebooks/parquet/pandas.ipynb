{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requisitos\n",
    "# Para rodar este notebook, você precisa ter os pacotes `pandas`, `pyarrow` e `parquet` instalados. Você pode instalar usando:\n",
    "%!pip install pandas pyarrow parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importar os pacotes necessários\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pyarrow  # Se necessário para operações com Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando um DataFrame de exemplo com diversos tipos de dados\n",
    "df = pd.DataFrame({\n",
    "    'Nome': ['Maria', 'John', 'Tonico', 'Mariane'],\n",
    "    'Idade': [25, 30, 35, np.nan],  # Incluindo um valor nulo\n",
    "    'Salario': [50000.50, 60000.75, 70000.00, 80000.25],\n",
    "    'Data_Admissao': pd.to_datetime(['2020-01-15', '2019-05-20', '2018-11-01', '2021-03-10']),\n",
    "    'Descrição': ['Desenvolvedora Python', 'Analista de Dados', 'Cientista de Dados', 'Gerente de Projetos com acentuação çãõ'],\n",
    "    'Status': ['Ativo', 'Inativo', 'Ativo', 'Ativo']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escrevendo o DataFrame para um arquivo Parquet usando pandas (com pyarrow como engine padrão)\n",
    "try:\n",
    "    df.to_parquet('dataset/df_pandas_pyarrow.parquet', engine='pyarrow') # Especificando o engine pyarrow\n",
    "    print(\"\\nArquivo Parquet 'dataset/df_pandas_pyarrow.parquet' escrito com sucesso usando pandas com engine pyarrow!\")\n",
    "\n",
    "    # df.to_parquet('dataset/df_pandas_fastparquet.parquet', engine='fastparquet') # Especificando o engine fastparquet\n",
    "    # print(\"\\nArquivo Parquet 'dataset/df_pandas_fastparquet.parquet' escrito com sucesso usando pandas com engine fastparquet!\")\n",
    "\n",
    "    # df.to_parquet('dataset/df_pandas.parquet') # Usando o engine padrão (pyarrow se instalado, senão fastparquet)\n",
    "    # print(\"\\nArquivo Parquet 'dataset/df_pandas.parquet' escrito com sucesso usando pandas com engine padrão!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao escrever o arquivo Parquet: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ler arquivo `Parquet` utilizando `Pandas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lendo o arquivo Parquet de volta usando pandas\n",
    "try:\n",
    "    df_lido = pd.read_parquet('dataset/df_pandas_pyarrow.parquet')\n",
    "    print(\"\\nDataFrame lido do arquivo Parquet:\")\n",
    "    display(df_lido)\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao ler o arquivo Parquet: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ler apenas colunas especificas em uma tabela utilizando `Pandas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lendo apenas colunas\n",
    "try:\n",
    "    # Lendo o arquivo Parquet\n",
    "    df = pd.read_parquet('dataset/df_pandas_pyarrow.parquet')\n",
    "\n",
    "    # 1. Lendo uma única coluna como uma Series (ex: 'Nome')\n",
    "    nomes = df['Nome']\n",
    "    print(\"\\nColuna 'Nome' como Series:\")\n",
    "    print(nomes)\n",
    "    print(type(nomes))\n",
    "\n",
    "    # 2. Lendo múltiplas colunas como um novo DataFrame (ex: 'Nome' e 'Salario')\n",
    "    dados_pessoais = df[['Nome', 'Salario']]\n",
    "    display(\"\\nColunas 'Nome' e 'Salario' como DataFrame:\")\n",
    "    print(dados_pessoais)\n",
    "    display(type(dados_pessoais))\n",
    "\n",
    "    # 3. Lendo uma coluna usando .loc (ex: 'Data_Admissao')\n",
    "    datas_admissao = df.loc[:, 'Data_Admissao']\n",
    "    print(\"\\nColuna 'Data_Admissao' usando .loc:\")\n",
    "    print(datas_admissao)\n",
    "    print(type(datas_admissao))\n",
    "\n",
    "    # 4. Lendo uma coluna usando .iloc (ex: segunda coluna, 'Idade')\n",
    "    idades = df.iloc[:, 1]\n",
    "    print(\"\\nSegunda coluna ('Idade') usando .iloc:\")\n",
    "    print(idades)\n",
    "    print(type(idades))\n",
    "\n",
    "    # 5. Iterando sobre os valores de uma coluna (ex: 'Descrição')\n",
    "    print(\"\\nIterando sobre a coluna 'Descrição':\")\n",
    "    for descricao in df['Descrição']:\n",
    "        print(descricao)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Erro: Arquivo 'dataset/df_pandas.parquet' não encontrado. Certifique-se de que o arquivo existe no diretório correto.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ocorreu um erro ao ler o arquivo Parquet: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adicionar uma nova coluna e seus valores a uma tabela com `Pandas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adicionando colunas ao DataFrame\n",
    "try:\n",
    "    # Lendo o arquivo Parquet\n",
    "    df = pd.read_parquet('dataset/df_pandas_pyarrow.parquet')\n",
    "\n",
    "    # 1. Adicionando uma nova coluna com um valor constante\n",
    "    df['Nova_Coluna_Constante'] = 'Valor Constante'\n",
    "    print(\"\\nDataFrame com nova coluna constante:\")\n",
    "    display(df)\n",
    "\n",
    "    # 2. Adicionando uma nova coluna com valores calculados com base em outras colunas\n",
    "    df['Salario_Anual'] = df['Salario'] * 12\n",
    "    print(\"\\nDataFrame com nova coluna 'Salario_Anual':\")\n",
    "    display(df)\n",
    "\n",
    "    # 3. Adicionando uma nova coluna com valores condicionais (usando np.where)\n",
    "    df['Status'] = np.where(df['Idade'] >= 30, 'Adulto', 'Jovem')\n",
    "    print(\"\\nDataFrame com nova coluna 'Status':\")\n",
    "    display(df)\n",
    "\n",
    "    # 4. Adicionando uma nova coluna com valores a partir de uma lista\n",
    "    nova_lista = ['A', 'B', 'C', 'D']\n",
    "    if len(nova_lista) == len(df): # importante verificar se o tamanho da lista é igual ao tamanho do dataframe\n",
    "        df['Nova_Coluna_Lista'] = nova_lista\n",
    "        print(\"\\nDataFrame com nova coluna a partir de uma lista:\")\n",
    "        display(df)\n",
    "    else:\n",
    "        print(\"\\nErro: O tamanho da lista não corresponde ao número de linhas do DataFrame.\")\n",
    "\n",
    "\n",
    "    # Salvando o DataFrame modificado de volta para um arquivo Parquet (sobrescreve o arquivo existente)\n",
    "    df.to_parquet('dataset/df_pandas_com_nova_coluna.parquet') # Salvando com outro nome para não sobrescrever o original\n",
    "    print(\"\\nDataFrame modificado salvo em 'dataset/df_pandas_com_nova_coluna.parquet'\")\n",
    "\n",
    "    # Lendo o arquivo Parquet modificado para verificar\n",
    "    df_modificado = pd.read_parquet('dataset/df_pandas_com_nova_coluna.parquet')\n",
    "    print(\"\\nDataFrame lido do novo arquivo Parquet:\")\n",
    "    display(df_modificado)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Erro: Arquivo 'dataset/df_pandas.parquet' não encontrado. Certifique-se de que o arquivo existe no diretório correto.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ocorreu um erro: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alterar um valor específico em uma coluna de uma tabela com `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alterando valores no DataFrame\n",
    "try:\n",
    "    # Lendo o arquivo Parquet\n",
    "    df = pd.read_parquet('dataset/df_pandas_com_nova_coluna.parquet')\n",
    "\n",
    "    print(\"\\nDataFrame original:\")\n",
    "    display(df)\n",
    "\n",
    "    # 1. Alterando um valor específico com base no índice da linha e nome da coluna (usando .loc)\n",
    "    df.loc[0, 'Nome'] = 'Novo Nome da Maria'  # Altera o nome na primeira linha (índice 0)\n",
    "    print(\"\\nDataFrame com nome alterado (usando .loc):\")\n",
    "    display(df)\n",
    "\n",
    "    # 2. Alterando valores com base em uma condição (usando .loc e indexação booleana)\n",
    "    df.loc[df['Idade'] == 30, 'Status'] = 'Adulto Experiente'  # Altera o status onde a idade é 30\n",
    "    print(\"\\nDataFrame com status alterado (usando .loc e condição):\")\n",
    "    display(df)\n",
    "\n",
    "    # 3. Alterando múltiplos valores com base em uma condição (usando .loc e indexação booleana)\n",
    "    df.loc[df['Salario_Anual'] > 900000, ['Status', 'Nova_Coluna_Lista']] = ['Profissional de Alta Renda', 'X']\n",
    "    print(\"\\nDataFrame com status e nova coluna alterados (usando .loc e condição):\")\n",
    "    display(df)\n",
    "\n",
    "    # 4. Alterando um valor usando .at (mais eficiente para acesso a um único valor)\n",
    "    df.at[2, 'Descrição'] = 'Cientista de Dados Sênior com acentuação çãõ' # Altera a descrição na terceira linha (índice 2)\n",
    "    print(\"\\nDataFrame com descrição alterada (usando .at):\")\n",
    "    display(df)\n",
    "\n",
    "    # 5. Usando np.where para alterar valores condicionalmente em toda a coluna\n",
    "    df['Status'] = np.where(df['Idade'] < 25, 'Jovem Adulto', df['Status']) # Altera o status para 'Jovem Adulto' onde a idade é menor que 25\n",
    "    print(\"\\nDataFrame com status alterado (usando np.where):\")\n",
    "    display(df)\n",
    "\n",
    "    # Salvando as alterações de volta para um arquivo Parquet\n",
    "    df.to_parquet('dataset/df_parquet_alterado.parquet') # Salvando com outro nome para não sobrescrever o original\n",
    "    print(\"\\nDataFrame modificado salvo em 'dataset/df_parquet_alterado.parquet'\")\n",
    "\n",
    "    # Lendo o arquivo Parquet modificado para verificar as alterações\n",
    "    df_alterado = pd.read_parquet('dataset/df_parquet_alterado.parquet')\n",
    "    print(\"\\nDataFrame lido do arquivo Parquet alterado:\")\n",
    "    display(df_alterado)\n",
    "\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Erro: Arquivo 'dataset/df_pandas_com_nova_coluna.parquet' não encontrado. Certifique-se de que o arquivo existe no diretório correto.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ocorreu um erro: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excluir coluna existente de uma tabela com `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Lendo o arquivo Parquet\n",
    "    df = pd.read_parquet('dataset/df_parquet_alterado.parquet')\n",
    "\n",
    "    print(\"\\nDataFrame original:\")\n",
    "    display(df)\n",
    "\n",
    "    # 1. Excluindo uma única coluna usando o método drop()\n",
    "    df = df.drop(columns=['Nova_Coluna_Constante'])  # Removendo a coluna 'Nova_Coluna_Constante'\n",
    "    print(\"\\nDataFrame com a coluna 'Nova_Coluna_Constante' excluída:\")\n",
    "    display(df)\n",
    "\n",
    "    # 2. Excluindo múltiplas colunas usando o método drop()\n",
    "    df = df.drop(columns=['Salario_Anual', 'Nova_Coluna_Lista'])  # Removendo as colunas 'Salario_Anual' e 'Nova_Coluna_Lista'\n",
    "    print(\"\\nDataFrame com as colunas 'Salario_Anual' e 'Nova_Coluna_Lista' excluídas:\")\n",
    "    display(df)\n",
    "\n",
    "    # 3. Excluindo uma coluna usando del (menos recomendado, mas funciona)\n",
    "    del df['Status']\n",
    "    print(\"\\nDataFrame com a coluna 'Status' excluída usando del:\")\n",
    "    display(df)\n",
    "\n",
    "    # Salvando o DataFrame modificado de volta para um arquivo Parquet\n",
    "    df.to_parquet('dataset/df_parquet_sem_colunas.parquet') # Salvando com outro nome para não sobrescrever o original\n",
    "    print(\"\\nDataFrame modificado salvo em 'dataset/df_parquet_sem_colunas.parquet'\")\n",
    "\n",
    "    # Lendo o arquivo Parquet modificado para verificar as alterações\n",
    "    df_sem_colunas = pd.read_parquet('dataset/df_parquet_sem_colunas.parquet')\n",
    "    print(\"\\nDataFrame lido do arquivo Parquet sem as colunas:\")\n",
    "    display(df_sem_colunas)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Erro: Arquivo 'dataset/df_parquet_alterado.parquet' não encontrado. Certifique-se de que o arquivo existe no diretório correto.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ocorreu um erro: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Particionando um DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Lendo o arquivo Parquet\n",
    "    df = pd.read_parquet('dataset/df_parquet_alterado.parquet')\n",
    "\n",
    "    # Criando o diretório de destino\n",
    "    output_dir = 'dataset/df_parquet_particionado'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    print(\"\\nDataFrame original:\")\n",
    "    display(df)\n",
    "except Exception as e:\n",
    "    print(f\"Ocorreu um erro: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Particionando por 'Idade'\n",
    "    df.to_parquet(\n",
    "        path=output_dir,\n",
    "        partition_cols=['Idade'],\n",
    "        engine='pyarrow'\n",
    "    )\n",
    "    print(f\"\\nDataFrame particionado por 'Idade' e salvo em '{output_dir}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ocorreu um erro: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Verificando os arquivos particionados\n",
    "    print(\"\\nArquivos particionados criados:\")\n",
    "    for root, dirs, files in os.walk(output_dir):\n",
    "        for file in files:\n",
    "            print(os.path.join(root, file))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Ocorreu um erro: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Lendo um subconjunto particionado (agora usando uma idade existente: 30)\n",
    "    idade_para_ler = 30  # Escolhe uma idade existente\n",
    "    caminho_particao = os.path.join(output_dir, f'Idade={idade_para_ler}')\n",
    "\n",
    "    # Verificando se o diretório da partição existe antes de tentar ler\n",
    "    if os.path.exists(caminho_particao):\n",
    "        df_particionado = pd.read_parquet(caminho_particao)\n",
    "        print(f\"\\nDataFrame particionado onde Idade == {idade_para_ler}:\")\n",
    "        display(df_particionado)\n",
    "    else:\n",
    "        print(f\"\\nPartição com Idade == {idade_para_ler} não encontrada.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ocorreu um erro: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Exemplo com iteração para ler todas as partições\n",
    "    print(\"\\nLendo todas as partições existentes:\")\n",
    "    for nome_diretorio in os.listdir(output_dir):\n",
    "        if nome_diretorio.startswith(\"Idade=\"):\n",
    "            caminho_particao = os.path.join(output_dir, nome_diretorio)\n",
    "            df_particionado = pd.read_parquet(caminho_particao)\n",
    "            print(f\"\\nConteúdo da partição {nome_diretorio}:\")\n",
    "            display(df_particionado)\n",
    "        elif nome_diretorio == \"__HIVE_DEFAULT_PARTITION__\":\n",
    "            print(f\"\\nPartição default {nome_diretorio} ignorada:\")\n",
    "        else:\n",
    "            print(f\"\\nDiretório {nome_diretorio} não é uma partição válida:\")\n",
    "except Exception as e:\n",
    "    print(f\"Ocorreu um erro: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Particionando por múltiplas colunas (Idade e Descrição)\n",
    "    output_dir_multiplas_colunas = 'dataset/df_parquet_particionado_multiplas_colunas'\n",
    "    os.makedirs(output_dir_multiplas_colunas, exist_ok=True)\n",
    "\n",
    "    df.to_parquet(\n",
    "        path=output_dir_multiplas_colunas,\n",
    "        partition_cols=['Idade', 'Descrição'],\n",
    "        engine='pyarrow'\n",
    "    )\n",
    "    print(f\"\\nDataFrame particionado por 'Idade' e 'Descrição' e salvo em '{output_dir_multiplas_colunas}'.\")\n",
    "    # Verificando os arquivos particionados criados\n",
    "    print(\"\\nArquivos particionados criados:\")\n",
    "    for root, dirs, files in os.walk(output_dir_multiplas_colunas):\n",
    "        for file in files:\n",
    "            print(os.path.join(root, file))\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Erro: Arquivo 'dataset/df_parquet_particionado_multiplas_colunas' não encontrado. Certifique-se de que o arquivo existe no diretório correto.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ocorreu um erro: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyArrow: Uma Solução Completa de Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instalar Parquet, Pandas e Pyarrow\n",
    "%!pip install parquet\n",
    "%!pip install pandas\n",
    "%!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importar os pacotes necessários\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.dataset as ds\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando um DataFrame de exemplo com diversos tipos de dados\n",
    "df = pd.DataFrame({\n",
    "    'Nome': ['Maria', 'John', 'Tonico', 'Mariane'],\n",
    "    'Idade': [25, 30, 35, np.nan],  # Incluindo um valor nulo\n",
    "    'Salario': [50000.50, 60000.75, 70000.00, 80000.25],\n",
    "    'Data_Admissao': pd.to_datetime(['2020-01-15', '2019-05-20', '2018-11-01', '2021-03-10']),\n",
    "    'Descrição': ['Desenvolvedora Python', 'Analista de Dados', 'Cientista de Dados', 'Gerente de Projetos com acentuação çãõ']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Arquivo Parquet 'dataset/df_pyarrow.parquet' escrito com sucesso usando pyarrow!\n"
     ]
    }
   ],
   "source": [
    "# Converta o DataFrame em uma Arrow Table\n",
    "df = pa.Table.from_pandas(df)\n",
    "\n",
    "# Escrevendo o DataFrame para um arquivo Parquet usando pyarrow\n",
    "try:\n",
    "    pq.write_table(df, 'dataset/df_pyarrow.parquet')\n",
    "    print(\"\\nArquivo Parquet 'dataset/df_pyarrow.parquet' escrito com sucesso usando pyarrow!\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao escrever o arquivo Parquet: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ler arquivo `Parquet` utilizando pyarrow\n",
    "Pode utilizar o método `read_table`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame lido completo com pyarrow:\n",
      "pyarrow.Table\n",
      "Nome: string\n",
      "Idade: double\n",
      "Salario: double\n",
      "Data_Admissao: timestamp[ns]\n",
      "Descrição: string\n",
      "----\n",
      "Nome: [[\"Maria\",\"John\",\"Tonico\",\"Mariane\"]]\n",
      "Idade: [[25,30,35,null]]\n",
      "Salario: [[50000.5,60000.75,70000,80000.25]]\n",
      "Data_Admissao: [[2020-01-15 00:00:00.000000000,2019-05-20 00:00:00.000000000,2018-11-01 00:00:00.000000000,2021-03-10 00:00:00.000000000]]\n",
      "Descrição: [[\"Desenvolvedora Python\",\"Analista de Dados\",\"Cientista de Dados\",\"Gerente de Projetos com acentuação çãõ\"]]\n"
     ]
    }
   ],
   "source": [
    "# Lendo o arquivo completo com pyarrow\n",
    "try:\n",
    "    # Leia o arquivo Parquet em uma Arrow Table\n",
    "    df = pq.read_table('dataset/df_pyarrow.parquet')\n",
    "    print(\"\\nDataFrame lido completo com pyarrow:\")\n",
    "    print(df)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"\\nErro: Arquivo Parquet não encontrado. Certifique-se de que o arquivo 'dataset/df_pyarrow.parquet' ou 'dataset/sample_fastparquet.parquet' existe no diretório atual.\")\n",
    "except pd.errors.ParserError as pe:\n",
    "    print(f\"\\nErro de parsing do Parquet: {pe}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nOutro erro ao ler o arquivo Parquet: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ler apenas colunas especificas em uma tabela utilizando pyarrow\n",
    "Pode utilizar o método `read_table`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tabela com colunas selecionadas (Nome e Salario) com pyarrow:\n",
      "pyarrow.Table\n",
      "Nome: string\n",
      "Salario: double\n",
      "----\n",
      "Nome: [[\"Maria\",\"John\",\"Tonico\",\"Mariane\"]]\n",
      "Salario: [[50000.5,60000.75,70000,80000.25]]\n"
     ]
    }
   ],
   "source": [
    "# Lendo apenas as colunas 'Nome' e 'Salário' com pyarrow\n",
    "try:\n",
    "    # Abre o arquivo Parquet\n",
    "    df_colunas_selecionadas = pq.read_table('dataset/df_pyarrow.parquet', columns=['Nome', 'Salario'])\n",
    "    print(\"\\nTabela com colunas selecionadas (Nome e Salario) com pyarrow:\")\n",
    "    print(df_colunas_selecionadas)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"\\nErro: Arquivo Parquet não encontrado. Certifique-se de que o arquivo 'dataset/df_pyarrow.parquet' existe no diretório atual.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nOutro erro ao ler o arquivo Parquet: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adicionar uma nova coluna e seus valores a uma tabela pyarrow\n",
    "\n",
    "Pode utilizar o método `add_column`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tabela com a nova coluna:\n",
      "      Nome  Idade   Salario Data_Admissao  \\\n",
      "0    Maria   25.0  50000.50    2020-01-15   \n",
      "1     John   30.0  60000.75    2019-05-20   \n",
      "2   Tonico   35.0  70000.00    2018-11-01   \n",
      "3  Mariane    NaN  80000.25    2021-03-10   \n",
      "\n",
      "                                Descrição   Status  \n",
      "0                   Desenvolvedora Python    Ativo  \n",
      "1                       Analista de Dados  Inativo  \n",
      "2                      Cientista de Dados    Ativo  \n",
      "3  Gerente de Projetos com acentuação çãõ  Inativo  \n",
      "\n",
      "Arquivo Parquet 'dataset/output_com_nova_coluna.parquet' escrito com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Criar uma nova coluna com pyarrow\n",
    "try:\n",
    "    nova_coluna_nome = 'Status'\n",
    "    nova_coluna = pa.array(['Ativo', 'Inativo', 'Ativo', 'Inativo'])\n",
    "\n",
    "    # Verifica se a coluna já existe\n",
    "    if nova_coluna_nome in df.column_names:\n",
    "         print(f\"\\nA coluna '{nova_coluna_nome}' já existe na tabela. Não será adicionada.\")\n",
    "    else:\n",
    "        # Adiciona a nova coluna à tabela\n",
    "        df = df.add_column(len(df.column_names), pa.field(nova_coluna_nome, nova_coluna.type), nova_coluna)\n",
    "\n",
    "        # Converte a tabela pyarrow para um DataFrame pandas para visualização\n",
    "        df_final = df.to_pandas()\n",
    "\n",
    "        print(\"\\nTabela com a nova coluna:\")\n",
    "        print(df_final)\n",
    "\n",
    "        # Escreve a tabela em um arquivo Parquet\n",
    "        pq.write_table(df, 'dataset/output_com_nova_coluna.parquet')\n",
    "\n",
    "        print(\"\\nArquivo Parquet 'dataset/output_com_nova_coluna.parquet' escrito com sucesso!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nErro ao adicionar a coluna e escrever o arquivo Parquet: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alterar um valor específico em uma coluna de uma tabela com pyarrow.\n",
    "\n",
    "Embora o processo seja um pouco diferente de como faríamos em um pandas.DataFrame diretamente. A tabela pyarrow é imutável, então você precisará criar um novo array com a modificação e substituir a coluna inteira."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Valor na coluna 'Salario' no índice '1' alterado para '65000' com sucesso.\n",
      "\n",
      "Tabela com o valor alterado:\n",
      "      Nome  Idade   Salario Data_Admissao  \\\n",
      "0    Maria   25.0  50000.50    2020-01-15   \n",
      "1     John   30.0  65000.00    2019-05-20   \n",
      "2   Tonico   35.0  70000.00    2018-11-01   \n",
      "3  Mariane    NaN  80000.25    2021-03-10   \n",
      "\n",
      "                                Descrição   Status  \n",
      "0                   Desenvolvedora Python    Ativo  \n",
      "1                       Analista de Dados  Inativo  \n",
      "2                      Cientista de Dados    Ativo  \n",
      "3  Gerente de Projetos com acentuação çãõ  Inativo  \n",
      "\n",
      "Arquivo Parquet 'dataset/output_valor_alterado.parquet' escrito com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Alterar valor de um indice da coluna com pyarrow\n",
    "try:\n",
    "\n",
    "    # Especificações da alteração\n",
    "    coluna_para_alterar = 'Salario'\n",
    "    indice_do_valor = 1  # Índice do valor a ser alterado (Bob)\n",
    "    novo_valor = 65000\n",
    "\n",
    "    # Verifica se a coluna existe\n",
    "    if coluna_para_alterar not in df.column_names:\n",
    "        print(f\"\\nErro: A coluna '{coluna_para_alterar}' não existe.\")\n",
    "    else:\n",
    "        # Verifica se o índice está dentro do alcance\n",
    "        coluna = df.column(coluna_para_alterar)\n",
    "        if indice_do_valor < 0 or indice_do_valor >= len(coluna):\n",
    "             print(f\"\\nErro: Índice '{indice_do_valor}' fora do alcance da coluna '{coluna_para_alterar}'.\")\n",
    "        else:\n",
    "            # Cria um novo array (cópia) com o valor alterado\n",
    "            novo_array = coluna.to_numpy().copy() # Usamos copy() aqui\n",
    "            novo_array[indice_do_valor] = novo_valor\n",
    "            novo_array = pa.array(novo_array)\n",
    "\n",
    "            # Substitui a coluna original pela nova coluna\n",
    "            df = df.set_column(df.column_names.index(coluna_para_alterar), pa.field(coluna_para_alterar, novo_array.type), novo_array)\n",
    "\n",
    "            print(f\"\\nValor na coluna '{coluna_para_alterar}' no índice '{indice_do_valor}' alterado para '{novo_valor}' com sucesso.\")\n",
    "\n",
    "            # Converte a tabela para um DataFrame pandas para visualização\n",
    "            df_final = df.to_pandas()\n",
    "            print(\"\\nTabela com o valor alterado:\")\n",
    "            print(df_final)\n",
    "\n",
    "\n",
    "            # Escreve a tabela em um novo arquivo Parquet\n",
    "            pq.write_table(df, 'dataset/output_valor_alterado.parquet')\n",
    "            print(\"\\nArquivo Parquet 'dataset/output_valor_alterado.parquet' escrito com sucesso!\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nErro ao alterar o valor e escrever o arquivo Parquet: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excluir uma coluna existente de uma tabela com pyarrow.\n",
    "\n",
    "Pode usar o método` remove_column`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A coluna 'Salario' foi removida com sucesso.\n",
      "\n",
      "Tabela sem a coluna:\n",
      "      Nome  Idade Data_Admissao                               Descrição  \\\n",
      "0    Maria   25.0    2020-01-15                   Desenvolvedora Python   \n",
      "1     John   30.0    2019-05-20                       Analista de Dados   \n",
      "2   Tonico   35.0    2018-11-01                      Cientista de Dados   \n",
      "3  Mariane    NaN    2021-03-10  Gerente de Projetos com acentuação çãõ   \n",
      "\n",
      "    Status  \n",
      "0    Ativo  \n",
      "1  Inativo  \n",
      "2    Ativo  \n",
      "3  Inativo  \n",
      "\n",
      "Erro ao remover a coluna e escrever o arquivo Parquet: module 'pyarrow.parquet' has no attribute 'df'\n"
     ]
    }
   ],
   "source": [
    "# Excluir uma coluna com pyarrow\n",
    "\n",
    "try:\n",
    "    # Nome da coluna a ser removida\n",
    "    coluna_a_remover = 'Salario'\n",
    "\n",
    "    # Verifica se a coluna existe antes de tentar remover\n",
    "    if coluna_a_remover in df.column_names:\n",
    "      # Remove a coluna da tabela\n",
    "        df = df.remove_column(df.column_names.index(coluna_a_remover))\n",
    "        print(f\"\\nA coluna '{coluna_a_remover}' foi removida com sucesso.\")\n",
    "\n",
    "      # Converte a tabela pyarrow para um DataFrame pandas para visualização\n",
    "        df_final = df.to_pandas()\n",
    "\n",
    "        print(\"\\nTabela sem a coluna:\")\n",
    "        print(df_final)\n",
    "\n",
    "        # Escreve a tabela em um novo arquivo Parquet\n",
    "        pq.df(df, 'dataset/output_sem_coluna.parquet')\n",
    "\n",
    "        print(\"\\nArquivo Parquet 'dataset/output_sem_coluna.parquet' escrito com sucesso!\")\n",
    "    else:\n",
    "        print(f\"\\nA coluna '{coluna_a_remover}' não existe na tabela. A remoção não será realizada.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nErro ao remover a coluna e escrever o arquivo Parquet: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Particionar arquivos Parquet\n",
    "\n",
    "É uma prática comum em data lakes para otimizar consultas, permitindo que os mecanismos de processamento leiam apenas os arquivos relevantes para uma consulta específica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Arquivo Parquet 'dataset/df_pyarrow.parquet' escrito com sucesso usando pyarrow!\n"
     ]
    }
   ],
   "source": [
    "# 1. Criando um DataFrame de exemplo com diversos tipos de dados\n",
    "df = pd.DataFrame({\n",
    "    'Nome': ['Maria', 'John', 'Tonico', 'Mariane'],\n",
    "    'Idade': [25, 30, 35, np.nan],  # Incluindo um valor nulo\n",
    "    'Salario': [50000.50, 60000.75, 70000.00, 80000.25],\n",
    "    'Data_Admissao': pd.to_datetime(['2020-01-15', '2019-05-20', '2018-11-01', '2021-03-10']),\n",
    "    'Descrição': ['Desenvolvedora Python', 'Analista de Dados', 'Cientista de Dados', 'Gerente de Projetos com acentuação çãõ'],\n",
    "    'Status': ['Ativo', 'Inativo', 'Ativo', 'Ativo']\n",
    "})\n",
    "\n",
    "# 2. Escrevendo o DataFrame para um arquivo Parquet usando pyarrow\n",
    "try:\n",
    "    table_inicial = pa.Table.from_pandas(df)\n",
    "    pq.write_table(table_inicial, 'dataset/df_pyarrow.parquet')\n",
    "    print(\"\\nArquivo Parquet 'dataset/df_pyarrow.parquet' escrito com sucesso usando pyarrow!\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nErro ao escrever o arquivo Parquet: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Arquivo Parquet 'dataset/df_pyarrow.parquet' lido com sucesso.\n",
      "\n",
      "Dados particionados por ['Status'] e salvos em 'dataset/partitioned_data' com sucesso.\n"
     ]
    }
   ],
   "source": [
    "# 3. Lendo o arquivo Parquet para uma Arrow Table\n",
    "try:\n",
    "    table_lido = pq.read_table('dataset/df_pyarrow.parquet')\n",
    "    df_lido = table_lido.to_pandas()\n",
    "    print(f\"\\nArquivo Parquet 'dataset/df_pyarrow.parquet' lido com sucesso.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nErro ao ler o arquivo Parquet: {e}\")\n",
    "\n",
    "# 4. Definindo o caminho base e coluna de particionamento\n",
    "base_path = 'dataset/partitioned_data'\n",
    "partition_cols = ['Status']\n",
    "\n",
    "# 4.1. Cria o diretório de particionamento, se não existir\n",
    "if not os.path.exists(base_path):\n",
    "    os.makedirs(base_path)\n",
    "\n",
    "# 5. Particionamento e escrita\n",
    "try:\n",
    "    # Convertendo o DataFrame para Arrow Table\n",
    "    table_para_particionar = pa.Table.from_pandas(df_lido)\n",
    "\n",
    "    pq.write_to_dataset(\n",
    "        table_para_particionar,\n",
    "        root_path=base_path,\n",
    "        partitioning=pa.dataset.partitioning(pa.schema([(\"Status\", pa.string())]), flavor=\"hive\"),\n",
    "        existing_data_behavior='overwrite_or_ignore'\n",
    "    )\n",
    "    print(f\"\\nDados particionados por {partition_cols} e salvos em '{base_path}' com sucesso.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nErro ao particionar e escrever os arquivos Parquet: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dados filtrados (Status = Ativo):\n",
      "      Nome  Idade   Salario Data_Admissao  \\\n",
      "0    Maria   25.0  50000.50    2020-01-15   \n",
      "1   Tonico   35.0  70000.00    2018-11-01   \n",
      "2  Mariane    NaN  80000.25    2021-03-10   \n",
      "\n",
      "                                Descrição Status  \n",
      "0                   Desenvolvedora Python  Ativo  \n",
      "1                      Cientista de Dados  Ativo  \n",
      "2  Gerente de Projetos com acentuação çãõ  Ativo  \n"
     ]
    }
   ],
   "source": [
    "# 6. Leitura com filtro utilizando pyarrow.dataset\n",
    "try:\n",
    "    # Lendo dados particionados diretamente como um dataset\n",
    "    dataset = ds.dataset(base_path, format=\"parquet\", partitioning=\"hive\")\n",
    "\n",
    "    # Aplicando filtro para 'Status = Ativo'\n",
    "    filtered_table = dataset.to_table(filter=ds.field('Status') == 'Ativo')\n",
    "    filtered_df = filtered_table.to_pandas()\n",
    "\n",
    "    print(\"\\nDados filtrados (Status = Ativo):\")\n",
    "    print(filtered_df)\n",
    "except Exception as e:\n",
    "    print(f\"\\nErro ao ler arquivos Parquet particionados: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Diretório dataset/partitioned_data removido\n"
     ]
    }
   ],
   "source": [
    "# 7. Limpeza (opcional)\n",
    "if os.path.exists(base_path):\n",
    "    for root, dirs, files in os.walk(base_path, topdown=False):\n",
    "        for name in files:\n",
    "            os.remove(os.path.join(root, name))\n",
    "        for name in dirs:\n",
    "            os.rmdir(os.path.join(root, name))\n",
    "    os.rmdir(base_path)\n",
    "    print(f\"\\nDiretório {base_path} removido\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Particionar um arquivo Parquet é útil para lidar com grandes volumes de dados, melhorando a eficiência de consultas e a organização. O particionamento divide os dados em subdiretórios baseados nos valores de uma ou mais colunas (e.g., Status), permitindo:\n",
    "\n",
    "- Consultas mais rápidas: Apenas as partições relevantes são lidas, reduzindo o tempo de leitura e uso de memória.\n",
    "- Escalabilidade: Permite o processamento paralelo, especialmente em sistemas distribuídos como Spark.\n",
    "- Organização: Facilita a navegação e o gerenciamento de dados, agrupando informações logicamente.\n",
    "\n",
    "Esses benefícios tornam o particionamento ideal para análises em larga escala e sistemas que processam dados frequentemente."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

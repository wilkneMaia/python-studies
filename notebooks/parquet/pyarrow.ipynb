{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyArrow: Uma Solução Completa de Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requisitos\n",
    "Para rodar este notebook, instale os pacotes necessários com o seguinte comando:\n",
    "```bash\n",
    "pip install pandas pyarrow parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importar os pacotes necessários\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.dataset as ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando um DataFrame de exemplo com diversos tipos de dados\n",
    "df = pd.DataFrame({\n",
    "    'Nome': ['Maria', 'John', 'Tonico', 'Mariane'],\n",
    "    'Idade': [25, 30, 35, np.nan],  # Incluindo um valor nulo\n",
    "    'Salario': [50000.50, 60000.75, 70000.00, 80000.25],\n",
    "    'Data_Admissao': pd.to_datetime(['2020-01-15', '2019-05-20', '2018-11-01', '2021-03-10']),\n",
    "    'Descrição': ['Desenvolvedora Python', 'Analista de Dados', 'Cientista de Dados', 'Gerente de Projetos com acentuação çãõ']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converta o DataFrame em uma Arrow Table\n",
    "# df = pa.Table.from_pandas(df)\n",
    "\n",
    "# Verifique os dados antes de converter para Arrow Table\n",
    "print(\"DataFrame original:\")\n",
    "display(df.head())  # Exibir as primeiras linhas do DataFrame\n",
    "\n",
    "# Converta o DataFrame em uma Arrow Table\n",
    "arrow_table = pa.Table.from_pandas(df)\n",
    "\n",
    "# Exibir a Arrow Table\n",
    "print(\"\\nArrow Table:\")\n",
    "display(arrow_table)\n",
    "\n",
    "# Escrevendo a Arrow Table para um arquivo Parquet usando pyarrow\n",
    "try:\n",
    "    pq.write_table(arrow_table, 'dataset/df_pyarrow.parquet')\n",
    "    print(\"\\nArquivo Parquet 'dataset/df_pyarrow.parquet' escrito com sucesso usando pyarrow!\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao escrever o arquivo Parquet: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ler arquivo `Parquet` utilizando pyarrow\n",
    "Pode utilizar o método `read_table`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lendo o arquivo completo com pyarrow\n",
    "try:\n",
    "    # Leia o arquivo Parquet em uma Arrow Table\n",
    "    df = pq.read_table('dataset/df_pyarrow.parquet')\n",
    "    print(\"\\nDataFrame lido completo com pyarrow:\")\n",
    "    print(df)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"\\nErro: Arquivo Parquet não encontrado. Certifique-se de que o arquivo 'dataset/df_pyarrow.parquet' ou 'dataset/sample_fastparquet.parquet' existe no diretório atual.\")\n",
    "except pd.errors.ParserError as pe:\n",
    "    print(f\"\\nErro de parsing do Parquet: {pe}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nOutro erro ao ler o arquivo Parquet: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ler apenas colunas especificas em uma tabela utilizando pyarrow\n",
    "Pode utilizar o método `read_table`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lendo apenas as colunas 'Nome' e 'Salário' com pyarrow\n",
    "try:\n",
    "    # Abre o arquivo Parquet\n",
    "    df_colunas_selecionadas = pq.read_table('dataset/df_pyarrow.parquet', columns=['Nome', 'Salario'])\n",
    "    print(\"\\nTabela com colunas selecionadas (Nome e Salario) com pyarrow:\")\n",
    "    display(df_colunas_selecionadas)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"\\nErro: Arquivo Parquet não encontrado. Certifique-se de que o arquivo 'dataset/df_pyarrow.parquet' existe no diretório atual.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nOutro erro ao ler o arquivo Parquet: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adicionar uma nova coluna e seus valores a uma tabela com `Pyarrow`\n",
    "\n",
    "Pode utilizar o método `add_column`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar uma nova coluna com pyarrow\n",
    "try:\n",
    "    nova_coluna_nome = 'Status'\n",
    "    nova_coluna = pa.array(['Ativo', 'Inativo', 'Ativo', 'Inativo'])\n",
    "\n",
    "    # Verifica se a coluna já existe\n",
    "    if nova_coluna_nome in df.column_names:\n",
    "         print(f\"\\nA coluna '{nova_coluna_nome}' já existe na tabela. Não será adicionada.\")\n",
    "    else:\n",
    "        # Adiciona a nova coluna à tabela\n",
    "        df = df.add_column(len(df.column_names), pa.field(nova_coluna_nome, nova_coluna.type), nova_coluna)\n",
    "\n",
    "        # Converte a tabela pyarrow para um DataFrame pandas para visualização\n",
    "        df_final = df.to_pandas()\n",
    "\n",
    "        print(\"\\nTabela com a nova coluna:\")\n",
    "        display(df_final)\n",
    "\n",
    "        # Escreve a tabela em um arquivo Parquet\n",
    "        pq.write_table(df, 'dataset/output_com_nova_coluna.parquet')\n",
    "\n",
    "        print(\"\\nArquivo Parquet 'dataset/output_com_nova_coluna.parquet' escrito com sucesso!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nErro ao adicionar a coluna e escrever o arquivo Parquet: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alterar um valor específico em uma coluna de uma tabela com pyarrow.\n",
    "\n",
    "Embora o processo seja um pouco diferente de como faríamos em um pandas.DataFrame diretamente. A tabela pyarrow é imutável, então você precisará criar um novo array com a modificação e substituir a coluna inteira."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alterar valor de um indice da coluna com pyarrow\n",
    "try:\n",
    "\n",
    "    # Especificações da alteração\n",
    "    coluna_para_alterar = 'Salario'\n",
    "    indice_do_valor = 1  # Índice do valor a ser alterado (Bob)\n",
    "    novo_valor = 65000\n",
    "\n",
    "    # Verifica se a coluna existe\n",
    "    if coluna_para_alterar not in df.column_names:\n",
    "        print(f\"\\nErro: A coluna '{coluna_para_alterar}' não existe.\")\n",
    "    else:\n",
    "        # Verifica se o índice está dentro do alcance\n",
    "        coluna = df.column(coluna_para_alterar)\n",
    "        if indice_do_valor < 0 or indice_do_valor >= len(coluna):\n",
    "             print(f\"\\nErro: Índice '{indice_do_valor}' fora do alcance da coluna '{coluna_para_alterar}'.\")\n",
    "        else:\n",
    "            # Cria um novo array (cópia) com o valor alterado\n",
    "            novo_array = coluna.to_numpy().copy() # Usamos copy() aqui\n",
    "            novo_array[indice_do_valor] = novo_valor\n",
    "            novo_array = pa.array(novo_array)\n",
    "\n",
    "            # Substitui a coluna original pela nova coluna\n",
    "            df = df.set_column(df.column_names.index(coluna_para_alterar), pa.field(coluna_para_alterar, novo_array.type), novo_array)\n",
    "\n",
    "            print(f\"\\nValor na coluna '{coluna_para_alterar}' no índice '{indice_do_valor}' alterado para '{novo_valor}' com sucesso.\")\n",
    "\n",
    "            # Converte a tabela para um DataFrame pandas para visualização\n",
    "            df_final = df.to_pandas()\n",
    "            print(\"\\nTabela com o valor alterado:\")\n",
    "            display(df_final)\n",
    "\n",
    "\n",
    "            # Escreve a tabela em um novo arquivo Parquet\n",
    "            pq.write_table(df, 'dataset/output_valor_alterado.parquet')\n",
    "            print(\"\\nArquivo Parquet 'dataset/output_valor_alterado.parquet' escrito com sucesso!\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nErro ao alterar o valor e escrever o arquivo Parquet: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excluir uma coluna existente de uma tabela com pyarrow.\n",
    "\n",
    "Pode usar o método` remove_column`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excluir uma coluna com pyarrow\n",
    "\n",
    "try:\n",
    "    # Nome da coluna a ser removida\n",
    "    coluna_a_remover = 'Salario'\n",
    "\n",
    "    # Verifica se a coluna existe antes de tentar remover\n",
    "    if coluna_a_remover in df.column_names:\n",
    "      # Remove a coluna da tabela\n",
    "        df = df.remove_column(df.column_names.index(coluna_a_remover))\n",
    "        print(f\"\\nA coluna '{coluna_a_remover}' foi removida com sucesso.\")\n",
    "\n",
    "      # Converte a tabela pyarrow para um DataFrame pandas para visualização\n",
    "        df_final = df.to_pandas()\n",
    "\n",
    "        print(\"\\nTabela sem a coluna:\")\n",
    "        display(df_final)\n",
    "\n",
    "        # Escreve a tabela em um novo arquivo Parquet\n",
    "        pq.df(df, 'dataset/output_sem_coluna.parquet')\n",
    "\n",
    "        print(\"\\nArquivo Parquet 'dataset/output_sem_coluna.parquet' escrito com sucesso!\")\n",
    "    else:\n",
    "        print(f\"\\nA coluna '{coluna_a_remover}' não existe na tabela. A remoção não será realizada.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nErro ao remover a coluna e escrever o arquivo Parquet: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Particionar arquivos Parquet\n",
    "\n",
    "É uma prática comum em data lakes para otimizar consultas, permitindo que os mecanismos de processamento leiam apenas os arquivos relevantes para uma consulta específica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Criando um DataFrame de exemplo com diversos tipos de dados\n",
    "df = pd.DataFrame({\n",
    "    'Nome': ['Maria', 'John', 'Tonico', 'Mariane'],\n",
    "    'Idade': [25, 30, 35, np.nan],  # Incluindo um valor nulo\n",
    "    'Salario': [50000.50, 60000.75, 70000.00, 80000.25],\n",
    "    'Data_Admissao': pd.to_datetime(['2020-01-15', '2019-05-20', '2018-11-01', '2021-03-10']),\n",
    "    'Descrição': ['Desenvolvedora Python', 'Analista de Dados', 'Cientista de Dados', 'Gerente de Projetos com acentuação çãõ'],\n",
    "    'Status': ['Ativo', 'Inativo', 'Ativo', 'Ativo']\n",
    "})\n",
    "\n",
    "# 2. Escrevendo o DataFrame para um arquivo Parquet usando pyarrow\n",
    "try:\n",
    "    table_inicial = pa.Table.from_pandas(df)\n",
    "    pq.write_table(table_inicial, 'dataset/df_pyarrow.parquet')\n",
    "    print(\"\\nArquivo Parquet 'dataset/df_pyarrow.parquet' escrito com sucesso usando pyarrow!\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nErro ao escrever o arquivo Parquet: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Lendo o arquivo Parquet para uma Arrow Table\n",
    "try:\n",
    "    table_lido = pq.read_table('dataset/df_pyarrow.parquet')\n",
    "    df_lido = table_lido.to_pandas()\n",
    "    print(f\"\\nArquivo Parquet 'dataset/df_pyarrow.parquet' lido com sucesso.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nErro ao ler o arquivo Parquet: {e}\")\n",
    "\n",
    "# 4. Definindo o caminho base e coluna de particionamento\n",
    "base_path = 'dataset/partitioned_data'\n",
    "partition_cols = ['Status']\n",
    "\n",
    "# 4.1. Cria o diretório de particionamento, se não existir\n",
    "if not os.path.exists(base_path):\n",
    "    os.makedirs(base_path)\n",
    "\n",
    "# 5. Particionamento e escrita\n",
    "try:\n",
    "    # Convertendo o DataFrame para Arrow Table\n",
    "    table_para_particionar = pa.Table.from_pandas(df_lido)\n",
    "\n",
    "    pq.write_to_dataset(\n",
    "        table_para_particionar,\n",
    "        root_path=base_path,\n",
    "        partitioning=pa.dataset.partitioning(pa.schema([(\"Status\", pa.string())]), flavor=\"hive\"),\n",
    "        existing_data_behavior='overwrite_or_ignore'\n",
    "    )\n",
    "    print(f\"\\nDados particionados por {partition_cols} e salvos em '{base_path}' com sucesso.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nErro ao particionar e escrever os arquivos Parquet: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Leitura com filtro utilizando pyarrow.dataset\n",
    "try:\n",
    "    # Lendo dados particionados diretamente como um dataset\n",
    "    dataset = ds.dataset(base_path, format=\"parquet\", partitioning=\"hive\")\n",
    "\n",
    "    # Aplicando filtro para 'Status = Ativo'\n",
    "    filtered_table = dataset.to_table(filter=ds.field('Status') == 'Ativo')\n",
    "    filtered_df = filtered_table.to_pandas()\n",
    "\n",
    "    print(\"\\nDados filtrados (Status = Ativo):\")\n",
    "    display(filtered_df)\n",
    "except Exception as e:\n",
    "    print(f\"\\nErro ao ler arquivos Parquet particionados: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Limpeza (opcional)\n",
    "if os.path.exists(base_path):\n",
    "    for root, dirs, files in os.walk(base_path, topdown=False):\n",
    "        for name in files:\n",
    "            os.remove(os.path.join(root, name))\n",
    "        for name in dirs:\n",
    "            os.rmdir(os.path.join(root, name))\n",
    "    os.rmdir(base_path)\n",
    "    print(f\"\\nDiretório {base_path} removido\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Particionar um arquivo Parquet é útil para lidar com grandes volumes de dados, melhorando a eficiência de consultas e a organização. O particionamento divide os dados em subdiretórios baseados nos valores de uma ou mais colunas (e.g., Status), permitindo:\n",
    "\n",
    "- Consultas mais rápidas: Apenas as partições relevantes são lidas, reduzindo o tempo de leitura e uso de memória.\n",
    "- Escalabilidade: Permite o processamento paralelo, especialmente em sistemas distribuídos como Spark.\n",
    "- Organização: Facilita a navegação e o gerenciamento de dados, agrupando informações logicamente.\n",
    "\n",
    "Esses benefícios tornam o particionamento ideal para análises em larga escala e sistemas que processam dados frequentemente."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

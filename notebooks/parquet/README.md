# üöÄ Material de Apoio por M√≥dulo

Este reposit√≥rio cont√©m diversos notebooks que abordam diferentes t√≥picos sobre compress√£o, manipula√ß√£o e particionamento de dados no formato **Parquet** utilizando **Pandas** e **PyArrow**. Abaixo, voc√™ encontrar√° os m√≥dulos organizados com descri√ß√µes e links para acessar diretamente cada notebook.

## **üöÄ Material de apoio por m√≥dulo**

| M√≥dulo | Descri√ß√£o | Link para o Notebook |
|--------|-----------|----------------------|
| **1 - Manipula√ß√£o de Dados com PyArrow** | Foca na biblioteca **PyArrow** para leitura, escrita e manipula√ß√£o de arquivos Parquet. Inclui t√≥picos como particionamento de dados. | [pyarrow.ipynb](./pyarrow.ipynb) |
| **2 - Manipula√ß√£o de Dados com Pandas** | Aborda o uso do **Pandas** para manipula√ß√£o de arquivos Parquet, com opera√ß√µes como leitura, escrita e modifica√ß√£o de tabelas. | [pandas.ipynb](./pandas.ipynb) |
| **3 - Compress√£o de Arquivos Parquet** | Explora a compress√£o de arquivos Parquet com algoritmos como **Snappy**, **Gzip**, **Brotli** e **Zstd**, comparando o desempenho e a efici√™ncia de compress√£o. | [compression.ipynb](./compression.ipynb) |

---

## Descri√ß√£o dos Notebooks

Este reposit√≥rio cont√©m tr√™s notebooks principais que abordam diferentes aspectos da manipula√ß√£o e compress√£o de arquivos Parquet, al√©m de t√©cnicas de manipula√ß√£o de dados utilizando **Pandas** e **PyArrow**.

### 1. **[pyarrow.ipynb](./pyarrow.ipynb)**

O notebook **PyArrow.ipynb** aborda as funcionalidades do **PyArrow**, uma biblioteca eficiente para manipula√ß√£o de dados em formato Parquet. Este notebook inclui opera√ß√µes de leitura, modifica√ß√£o e escrita de arquivos Parquet, al√©m de introduzir o conceito de **particionamento de dados**.

#### Se√ß√µes do Notebook:
- **Ler arquivo Parquet utilizando PyArrow**: Como usar o PyArrow para ler arquivos Parquet.
- **Ler apenas colunas espec√≠ficas em uma tabela**: Sele√ß√£o de colunas de uma tabela Parquet com PyArrow.
- **Adicionar uma nova coluna e seus valores a uma tabela**: Como modificar tabelas Parquet com PyArrow.
- **Alterar um valor espec√≠fico em uma coluna de uma tabela**: Modificar valores em tabelas PyArrow.
- **Excluir uma coluna existente de uma tabela**: Como remover colunas de tabelas Parquet.
- **Particionar arquivos Parquet**: O particionamento de arquivos Parquet √© explicado, destacando a import√¢ncia de particionar dados em grandes volumes, especialmente para an√°lise de dados em sistemas distribu√≠dos.

### 2. **[pandas.ipynb](./pandas.ipynb)**

O notebook **Pandas.ipynb** foca em diversas opera√ß√µes de manipula√ß√£o de dados utilizando o **Pandas**, como leitura e escrita de arquivos Parquet, al√©m de manipula√ß√µes espec√≠ficas em tabelas.

#### Se√ß√µes do Notebook:
- **Ler arquivo Parquet utilizando Pandas**: Como carregar arquivos Parquet em um DataFrame do Pandas.
- **Ler apenas colunas espec√≠ficas em uma tabela**: Como selecionar colunas espec√≠ficas durante a leitura de arquivos.
- **Adicionar uma nova coluna e seus valores a uma tabela**: Mostra como adicionar colunas e preencher seus valores.
- **Alterar um valor espec√≠fico em uma coluna de uma tabela**: Manipula√ß√£o de dados em colunas j√° existentes.
- **Excluir coluna existente de uma tabela**: Como remover colunas de um DataFrame.
- **Particionando um DataFrame**: Estrat√©gias para particionar grandes DataFrames em subconjuntos.

### 3. **[compression.ipynb](./compression.ipynb)**

Este notebook explora o processo de **compress√£o de arquivos Parquet** com diferentes algoritmos, tais como **Snappy**, **Gzip**, **Brotli** e **Zstd**. O foco est√° em comparar a compress√£o de arquivos CSV em Parquet utilizando esses algoritmos, avaliando o desempenho de compress√£o, tempo de execu√ß√£o e taxa de redu√ß√£o de tamanho dos arquivos.

#### Se√ß√µes do Notebook:
- **Requisitos**: Instala√ß√£o dos pacotes necess√°rios.
- **Compress√£o de arquivo CSV em Parquet**: Converte um arquivo CSV para o formato Parquet.
- **Compress√£o de arquivo Parquet com Snappy**: Demonstrando o uso de compress√£o Snappy.
- **Compara√ß√£o de compress√£o em arquivos Parquet**: Comparando os algoritmos **snappy**, **gzip**, **brotli** e **zstd**.
- **Uso de Compress√£o Autom√°tica com o PyArrow**: Usando compress√£o autom√°tica no PyArrow.
- **Uso de Compress√£o Snappy e Compacta√ß√£o de Colunas Individualmente**: Aplicando compress√µes diferentes em colunas individuais.
- **Script de Compara√ß√£o de Compress√µes Parquet**: Compara√ß√£o de v√°rios algoritmos de compress√£o para decidir qual √© o mais eficiente.

---

## Como Rodar os Notebooks

1. **Instalar as depend√™ncias**:

   Antes de rodar os notebooks, instale os pacotes necess√°rios com o seguinte comando:

   ```bash
   pip install pandas pyarrow parquet
   ```

# Parquet: Acelere sua An√°lise de Dados e Reduza Custos em Nuvem

Em um mundo onde dados s√£o o novo petr√≥leo, a efici√™ncia no processamento se torna crucial. O formato Parquet surge como uma solu√ß√£o poderosa para otimizar a an√°lise de grandes volumes de dados, oferecendo ganhos significativos em performance e redu√ß√£o de custos. Imagine uma planilha com v√°rias colunas. Em um formato linha a linha (como CSV), voc√™ l√™ a linha inteira, mesmo que precise apenas de uma ou duas colunas. Com o Parquet, voc√™ l√™ apenas as colunas necess√°rias, como se estivesse "cortando" a tabela verticalmente e pegando apenas as partes relevantes. Isso economiza tempo e recursos.

## O que √© Parquet?

Parquet √© um formato de armazenamento de dados colunar, open-source, projetado para otimizar a leitura e o processamento de grandes volumes de dados. Diferentemente de formatos tradicionais como CSV, que armazenam dados linha a linha, o Parquet organiza as informa√ß√µes por colunas. Essa simples mudan√ßa tem um impacto enorme no desempenho de consultas e an√°lises.

## Por que o Parquet √© t√£o relevante?

### Armazenamento Colunar: A Base da Efici√™ncia

A principal diferen√ßa entre o Parquet e formatos tradicionais como CSV ou JSON √© a forma como os dados s√£o armazenados. Em vez de armazenar os dados linha a linha, o Parquet organiza as informa√ß√µes por colunas. Isso reduz drasticamente o tempo de leitura e o uso de recursos, especialmente em datasets com muitas colunas.

### Compress√£o Otimizada: Reduza Custos e Acelere suas An√°lises

O Parquet n√£o apenas organiza os dados de forma inteligente, mas tamb√©m os comprime de maneira eficaz. Ele utiliza algoritmos de compress√£o otimizados, como Snappy, Gzip e LZO, que reduzem o tamanho dos arquivos sem perda significativa de desempenho. Arquivos menores significam:

- **Menos espa√ßo em disco:** Redu√ß√£o de custos de armazenamento, especialmente em ambientes de nuvem.
- **Transfer√™ncias mais r√°pidas:** Acelera√ß√£o do processamento e da transfer√™ncia de dados pela rede.
- **Menor uso de recursos:** Redu√ß√£o do consumo de CPU e mem√≥ria durante a leitura e o processamento.

### Column Pruning: Leitura Seletiva de Colunas

Uma das caracter√≠sticas mais poderosas do Parquet √© a capacidade de realizar "column pruning" (poda de colunas). Isso significa que, durante uma consulta ou an√°lise, o sistema s√≥ precisa ler as colunas realmente necess√°rias, ignorando as demais. Em um dataset com muitas colunas, isso pode resultar em uma economia significativa de tempo e recursos.

### Schema: Organiza√ß√£o e Facilidade de Uso

O Parquet armazena o schema (estrutura) dos dados junto com os pr√≥prios dados. Isso torna a leitura e interpreta√ß√£o das informa√ß√µes muito mais simples e eficiente. O schema garante que os dados sejam lidos corretamente e facilita a integra√ß√£o com diferentes ferramentas e plataformas.

### Impacto na Performance e Economia de Custos

Ao combinar o armazenamento colunar, a compress√£o otimizada, o column pruning e o suporte a schema, o Parquet oferece uma melhoria significativa na performance de queries e an√°lises. Isso se traduz em menor tempo de processamento, menor uso de recursos e, consequentemente, economia de custos, especialmente em ambientes de nuvem.

## Parquet vs. Outros Formatos: Qual Escolher?

Para entender melhor o valor do Parquet, vamos compar√°-lo com outros formatos:

| Caracter√≠stica        | Parquet                 | CSV                     | JSON                    |
| ---------------------- | ----------------------- | ----------------------- | ----------------------- |
| Armazenamento          | Colunar                 | Linha a Linha           | Linha a Linha           |
| Compress√£o             | Alta                    | Nenhuma                 | Baixa                   |
| Schema                 | Suportado               | N√£o suportado           | Suportado (impl√≠cito)    |
| Efici√™ncia para an√°lise | Alta                    | Baixa                   | Baixa                   |
| Ideal para             | Grandes datasets, an√°lises | Pequenos datasets, troca de dados | APIs, dados semiestruturados |

## Lendo e Escrevendo Arquivos Parquet em Python

### Criando um DataFrame de Amostra

Primeiro, vamos criar um DataFrame de amostra com diferentes tipos de dados para demonstrar a versatilidade do Parquet:

```python
import pandas as pd
import numpy as np

# Criando um DataFrame de exemplo com diversos tipos de dados
data = {
    'Nome': ['Maria', 'John', 'Tonico', 'Mariane'],
    'Idade': [25, 30, 35, np.nan],  # Incluindo um valor nulo
    'Sal√°rio': [50000.50, 60000.75, 70000.00, 80000.25],
    'Data_Admiss√£o': pd.to_datetime(['2020-01-15', '2019-05-20', '2018-11-01', '2021-03-10']),
    'Descri√ß√£o': ['Desenvolvedora Python', 'Analista de Dados', 'Cientista de Dados', 'Gerente de Projetos com acentua√ß√£o √ß√£√µ']
}
df = pd.DataFrame(data)

print("DataFrame Original:")
print(df)
```

### Escrevendo Arquivos Parquet

O Pandas oferece suporte nativo para escrita de arquivos Parquet, simplificando o processo. O par√¢metro engine permite escolher o motor de escrita. O motor pyarrow √© geralmente recomendado para melhor performance, compatibilidade com mais tipos de dados e suporte a compress√£o. O fastparquet √© uma alternativa que pode ser mais r√°pida em alguns casos.

Vamos demonstrar a escrita com pyarrow e compress√£o snappy (uma boa op√ß√£o para um bom equil√≠brio entre compress√£o e velocidade) e tamb√©m com fastparquet:

```python
# Escrevendo o DataFrame para um arquivo Parquet usando pyarrow (recomendado) e compress√£o snappy
try:
    df.to_parquet('meu_arquivo.parquet', engine='pyarrow', compression='snappy')
    print("\nArquivo Parquet 'meu_arquivo.parquet' escrito com sucesso usando pyarrow e compress√£o snappy!")

    df.to_parquet('meu_arquivo_fastparquet.parquet', engine='fastparquet')
    print("\nArquivo Parquet 'meu_arquivo_fastparquet.parquet' escrito com sucesso usando fastparquet!")
except Exception as e:
    print(f"Erro ao escrever o arquivo Parquet: {e}")
```

### Lendo Arquivos Parquet

Agora, vamos ler os arquivos Parquet que acabamos de criar. Demonstraremos como ler o arquivo completo e como ler apenas colunas espec√≠ficas (column pruning), utilizando diferentes engines.

```python
# Lendo o arquivo Parquet
try:
    # Lendo o arquivo completo com pyarrow (recomendado)
    df_lido_pyarrow = pd.read_parquet('meu_arquivo.parquet', engine='pyarrow')
    print("\nDataFrame lido completo com pyarrow:")
    print(df_lido_pyarrow)

    # Lendo o arquivo completo com fastparquet
    df_lido_fastparquet = pd.read_parquet('meu_arquivo_fastparquet.parquet', engine='fastparquet')
    print("\nDataFrame lido completo com fastparquet:")
    print(df_lido_fastparquet)

    # Lendo apenas as colunas 'Nome' e 'Sal√°rio' com pyarrow (column pruning)
    df_colunas_selecionadas = pd.read_parquet('meu_arquivo.parquet', columns=['Nome', 'Sal√°rio'], engine='pyarrow')
    print("\nDataFrame com colunas selecionadas (Nome e Sal√°rio) com pyarrow:")
    print(df_colunas_selecionadas)

except FileNotFoundError:
    print("\nErro: Arquivo Parquet n√£o encontrado. Certifique-se de que o arquivo 'meu_arquivo.parquet' ou 'meu_arquivo_fastparquet.parquet' existe no diret√≥rio atual.")
except pd.errors.ParserError as pe:
    print(f"\nErro de parsing do Parquet: {pe}")
except Exception as e:
    print(f"\nOutro erro ao ler o arquivo Parquet: {e}")
```

## Conclus√£o

O Parquet √© uma ferramenta poderosa para qualquer profissional que trabalha com an√°lise de dados, oferecendo ganhos significativos em performance e economia de custos. Experimente utiliz√°-lo em seus projetos e veja a diferen√ßa! Compartilhe nos coment√°rios suas experi√™ncias com o Parquet e quais outros formatos voc√™ utiliza.
